{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Task 4",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "I = [i for i in range(41)]\nA = [i for i in range(11)]\nS = [(i, a) for i in I for a in A]\nlen(S)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 2,
          "output_type": "execute_result",
          "data": {
            "text/plain": "451"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": "# TASK 4\n\nimport numpy as np\nimport statistics,math\n\n# Supply chain homework Question V(5)\n\nnRuns = pow(10,4); #number of reps\nnSteps = 10000; #duration / length of simulation\n\ns0 = 40; #initial state\ngamma = 0.9 #discounting factor\ntotalReward = 0;\ntotalReward2 = 0;\nTotalRewardMATR = np.zeros(nRuns)\n\nfor i in range(1,nRuns):\n    s = s0;\n    reward = 0;\n\n    for j in range(1, nSteps):\n        demand = np.random.binomial(125, 0.2)\n        metDemand = min(s, demand); #compute met demand\n        unmetDemand = max((demand-s), 0); #compute unmet demand\n        compute_replenish=4\n        reward = reward+pow(gamma,(j-1))*(  metDemand*2000 - 150*unmetDemand - 400*compute_replenish); #from which we can compute the incurred reward\n        s = min((s-metDemand+4), 40); #update the new inventory level\n\n    TotalRewardMATR[i] = reward;\n    totalReward = totalReward + reward; #add observation to the cumulative estimate\n    totalReward2 = totalReward2 + pow(reward,2)\n\ntotalReward = totalReward/(nRuns-1) #get estimate for 1st moment\ntotalReward2 = totalReward2/(nRuns-1) #get estimate for 2nd moment\nstdMean = statistics.stdev(TotalRewardMATR)\n\nprint(\"for sample size = \", nRuns, \"the estimated quantity is equal to\",statistics.mean(TotalRewardMATR))\nprint(\"sample variance = \",statistics.variance(TotalRewardMATR))\nprint(\"confidence interval = (\",statistics.mean(TotalRewardMATR)-\n1.96*math.sqrt(statistics.variance(TotalRewardMATR)/\nnRuns),\",\",statistics.mean(TotalRewardMATR)\n+1.96*math.sqrt(statistics.variance(TotalRewardMATR)/nRuns),\")\") #Confidence Interval\nprint(\"half width of the confidence interval = \",\n1.96*math.sqrt(statistics.variance(TotalRewardMATR)/nRuns))\n\nprint(\"Estimate for V(40): \",totalReward) #print value function estimate\nprint(\" 95% Confidence interval V(40) lb: \", statistics.mean(TotalRewardMATR) -1.96*statistics.stdev(TotalRewardMATR)/math.sqrt(nRuns) ) #print the corresponding confidence interval\nprint(\" 95% Confidence interval V(40) ub: \", statistics.mean(TotalRewardMATR) +1.96*statistics.stdev(TotalRewardMATR)/math.sqrt(nRuns) ) #print the corresponding confidence interval\n\n\n#print(TotalRewardMATR)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# Making reward matrix for other tasks",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport math\nimport statistics,scipy\n#from statistics import sum\n\n# Define constants\nc = 400  # Cost of purchasing an item\np = 2000  # Price of selling an item\nr = 150  # Holding cost per unsold item\n\narray = np.zeros([41,11])\nfor i in range(len(array)):\n    for j in range(len(array[i])):\n            temp_array=[]\n            for k in range(125):\n                # Calculate the future inventory after placing the replenishment order\n                future_inventory = max(i -k,0)+j\n        \n                # Calculate the number of sold items\n                sold_items = min(i, k)\n        \n                # Calculate the purchase cost\n                purchase_cost = c * (sold_items)\n        \n                # Calculate the revenue from sales\n                revenue = p * sold_items\n        \n                # Calculate the holding cost for unsold items\n                holding_cost = r * (future_inventory - sold_items)\n        \n                # Calculate the total reward\n                total_reward = purchase_cost + revenue - holding_cost\n        \n                # Update the reward matrix\n                \n                \n                temp_array.append(scipy.stats.binom.pmf(k,125,0.2) * total_reward)\n\n                \n            array[i][j]=sum(temp_array)\nreward_matrix=array",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\n\n# Initialize Markov Decision Process model\nmaxCapacity = 40\nactions = range(0,10+1) #action space\nstates = range(0,maxCapacity+1) #state space\ndemands = range(0,41) #demand space\nrewards = reward_matrix  # Direct rewards per state\ngamma = 0.9  # discount factor\n\nmax_iter = 5\ndelta = 1e-400 \nV = np.zeros([41,11])\npi = np.zeros([41,11])\n\n\n# Start value iteration\nfor i in range(max_iter):\n      # Initialize max difference\n    V_new = np.zeros([41,11]) # Initialize values\n    for s in states:\n        max_diff = 0\n        for act in actions:\n            \n            for a in actions:\n                max_val = 0\n                # Compute state value\n                val = rewards[s][act]  # Get direct reward\n                for demand in demands:\n                    metDemand = min(s, demand); #compute met demand\n                    unmetDemand = max((demand-s), 0); #compute unmet demand\n    \n                    s_next = min(s-metDemand+a, maxCapacity) #update state\n    \n                    val += (1/5) * (gamma * V[s_next][act])  # Add discounted downstream values\n    \n                # Store value best action so far\n                max_val = max(max_val, val)\n    \n                # Update best policy\n                if V[s][act] < val:\n                    pi[s][act] = actions[a]  # Store action with highest value\n    \n            V_new[s][act] = max_val  # Update value with highest value\n            max_diff = max(max_diff, abs(V[s][act] - V_new[s][act]))\n\n    # Update value functions\n    V = V_new\n\n    # If diff smaller than threshold delta for all states, algorithm terminates\n    if max_diff < delta:\n        break\nprint(V)\nprint(pi)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[[1.03637363e+08 1.03122688e+08 1.02608014e+08 1.02093339e+08\n  1.01578665e+08 1.01063991e+08 1.00549316e+08 1.00034642e+08\n  9.95199675e+07 9.90052932e+07 9.84906188e+07]\n [1.03649746e+08 1.03135072e+08 1.02620397e+08 1.02105723e+08\n  1.01591049e+08 1.01076374e+08 1.00561700e+08 1.00047025e+08\n  9.95323511e+07 9.90176767e+07 9.85030023e+07]\n [1.03673048e+08 1.03158373e+08 1.02643699e+08 1.02129025e+08\n  1.01614350e+08 1.01099676e+08 1.00585002e+08 1.00070327e+08\n  9.95556528e+07 9.90409784e+07 9.85263040e+07]\n [1.03708383e+08 1.03193709e+08 1.02679034e+08 1.02164360e+08\n  1.01649686e+08 1.01135011e+08 1.00620337e+08 1.00105663e+08\n  9.95909882e+07 9.90763138e+07 9.85616394e+07]\n [1.03756898e+08 1.03242223e+08 1.02727549e+08 1.02212874e+08\n  1.01698200e+08 1.01183526e+08 1.00668851e+08 1.00154177e+08\n  9.96395025e+07 9.91248282e+07 9.86101538e+07]\n [1.03819765e+08 1.03305090e+08 1.02790416e+08 1.02275742e+08\n  1.01761067e+08 1.01246393e+08 1.00731719e+08 1.00217044e+08\n  9.97023698e+07 9.91876954e+07 9.86730211e+07]\n [1.03898187e+08 1.03383513e+08 1.02868838e+08 1.02354164e+08\n  1.01839489e+08 1.01324815e+08 1.00810141e+08 1.00295466e+08\n  9.97807919e+07 9.92661175e+07 9.87514431e+07]\n [1.03993393e+08 1.03478718e+08 1.02964044e+08 1.02449370e+08\n  1.01934695e+08 1.01420021e+08 1.00905347e+08 1.00390672e+08\n  9.98759978e+07 9.93613234e+07 9.88466490e+07]\n [1.04106639e+08 1.03591964e+08 1.03077290e+08 1.02562615e+08\n  1.02047941e+08 1.01533267e+08 1.01018592e+08 1.00503918e+08\n  9.99892435e+07 9.94745691e+07 9.89598947e+07]\n [1.04239207e+08 1.03724532e+08 1.03209858e+08 1.02695183e+08\n  1.02180509e+08 1.01665835e+08 1.01151160e+08 1.00636486e+08\n  1.00121811e+08 9.96071371e+07 9.90924627e+07]\n [1.04392406e+08 1.03877731e+08 1.03363057e+08 1.02848383e+08\n  1.02333708e+08 1.01819034e+08 1.01304360e+08 1.00789685e+08\n  1.00275011e+08 9.97603364e+07 9.92456621e+07]\n [1.04567536e+08 1.04052861e+08 1.03538187e+08 1.03023512e+08\n  1.02508838e+08 1.01994164e+08 1.01479489e+08 1.00964815e+08\n  1.00450140e+08 9.99354661e+07 9.94207917e+07]\n [1.04765885e+08 1.04251210e+08 1.03736536e+08 1.03221862e+08\n  1.02707187e+08 1.02192513e+08 1.01677839e+08 1.01163164e+08\n  1.00648490e+08 1.00133815e+08 9.96191411e+07]\n [1.04988735e+08 1.04474061e+08 1.03959386e+08 1.03444712e+08\n  1.02930037e+08 1.02415363e+08 1.01900689e+08 1.01386014e+08\n  1.00871340e+08 1.00356665e+08 9.98419911e+07]\n [1.05237359e+08 1.04722685e+08 1.04208011e+08 1.03693336e+08\n  1.03178662e+08 1.02663987e+08 1.02149313e+08 1.01634639e+08\n  1.01119964e+08 1.00605290e+08 1.00090616e+08]\n [1.05513026e+08 1.04998352e+08 1.04483677e+08 1.03969003e+08\n  1.03454329e+08 1.02939654e+08 1.02424980e+08 1.01910305e+08\n  1.01395631e+08 1.00880957e+08 1.00366282e+08]\n [1.05816999e+08 1.05302325e+08 1.04787650e+08 1.04272976e+08\n  1.03758301e+08 1.03243627e+08 1.02728953e+08 1.02214278e+08\n  1.01699604e+08 1.01184929e+08 1.00670255e+08]\n [1.06150538e+08 1.05635863e+08 1.05121189e+08 1.04606515e+08\n  1.04091840e+08 1.03577166e+08 1.03062492e+08 1.02547817e+08\n  1.02033143e+08 1.01518468e+08 1.01003794e+08]\n [1.06514901e+08 1.06000226e+08 1.05485552e+08 1.04970878e+08\n  1.04456203e+08 1.03941529e+08 1.03426854e+08 1.02912180e+08\n  1.02397506e+08 1.01882831e+08 1.01368157e+08]\n [1.06911344e+08 1.06396670e+08 1.05881995e+08 1.05367321e+08\n  1.04852647e+08 1.04337972e+08 1.03823298e+08 1.03308623e+08\n  1.02793949e+08 1.02279275e+08 1.01764600e+08]\n [1.07341124e+08 1.06826450e+08 1.06311776e+08 1.05797101e+08\n  1.05282427e+08 1.04767752e+08 1.04253078e+08 1.03738404e+08\n  1.03223729e+08 1.02709055e+08 1.02194381e+08]\n [1.07804186e+08 1.07289512e+08 1.06774837e+08 1.06260163e+08\n  1.05745488e+08 1.05230814e+08 1.04716140e+08 1.04201465e+08\n  1.03686791e+08 1.03172117e+08 1.02657442e+08]\n [1.08300478e+08 1.07785804e+08 1.07271129e+08 1.06756455e+08\n  1.06241781e+08 1.05727106e+08 1.05212432e+08 1.04697758e+08\n  1.04183083e+08 1.03668409e+08 1.03153734e+08]\n [1.08829958e+08 1.08315283e+08 1.07800609e+08 1.07285935e+08\n  1.06771260e+08 1.06256586e+08 1.05741911e+08 1.05227237e+08\n  1.04712563e+08 1.04197888e+08 1.03683214e+08]\n [1.09392592e+08 1.08877917e+08 1.08363243e+08 1.07848569e+08\n  1.07333894e+08 1.06819220e+08 1.06304545e+08 1.05789871e+08\n  1.05275197e+08 1.04760522e+08 1.04245848e+08]\n [1.09988360e+08 1.09473686e+08 1.08959012e+08 1.08444337e+08\n  1.07929663e+08 1.07414988e+08 1.06900314e+08 1.06385640e+08\n  1.05870965e+08 1.05356291e+08 1.04841616e+08]\n [1.10617257e+08 1.10102583e+08 1.09587908e+08 1.09073234e+08\n  1.08558560e+08 1.08043885e+08 1.07529211e+08 1.07014537e+08\n  1.06499862e+08 1.05985188e+08 1.05470513e+08]\n [1.11279289e+08 1.10764615e+08 1.10249941e+08 1.09735266e+08\n  1.09220592e+08 1.08705917e+08 1.08191243e+08 1.07676569e+08\n  1.07161894e+08 1.06647220e+08 1.06132545e+08]\n [1.11974474e+08 1.11459800e+08 1.10945126e+08 1.10430451e+08\n  1.09915777e+08 1.09401102e+08 1.08886428e+08 1.08371754e+08\n  1.07857079e+08 1.07342405e+08 1.06827730e+08]\n [1.12702838e+08 1.12188163e+08 1.11673489e+08 1.11158815e+08\n  1.10644140e+08 1.10129466e+08 1.09614791e+08 1.09100117e+08\n  1.08585443e+08 1.08070768e+08 1.07556094e+08]\n [1.13464410e+08 1.12949735e+08 1.12435061e+08 1.11920386e+08\n  1.11405712e+08 1.10891038e+08 1.10376363e+08 1.09861689e+08\n  1.09347015e+08 1.08832340e+08 1.08317666e+08]\n [1.14225858e+08 1.13711183e+08 1.13196509e+08 1.12681834e+08\n  1.12167160e+08 1.11652486e+08 1.11137811e+08 1.10623137e+08\n  1.10108463e+08 1.09593788e+08 1.09079114e+08]\n [1.14987211e+08 1.14472536e+08 1.13957862e+08 1.13443188e+08\n  1.12928513e+08 1.12413839e+08 1.11899164e+08 1.11384490e+08\n  1.10869816e+08 1.10355141e+08 1.09840467e+08]\n [1.15748494e+08 1.15233820e+08 1.14719145e+08 1.14204471e+08\n  1.13689797e+08 1.13175122e+08 1.12660448e+08 1.12145773e+08\n  1.11631099e+08 1.11116425e+08 1.10601750e+08]\n [1.16509728e+08 1.15995054e+08 1.15480380e+08 1.14965705e+08\n  1.14451031e+08 1.13936357e+08 1.13421682e+08 1.12907008e+08\n  1.12392333e+08 1.11877659e+08 1.11362985e+08]\n [1.17270930e+08 1.16756255e+08 1.16241581e+08 1.15726906e+08\n  1.15212232e+08 1.14697558e+08 1.14182883e+08 1.13668209e+08\n  1.13153535e+08 1.12638860e+08 1.12124186e+08]\n [1.18032109e+08 1.17517435e+08 1.17002760e+08 1.16488086e+08\n  1.15973412e+08 1.15458737e+08 1.14944063e+08 1.14429389e+08\n  1.13914714e+08 1.13400040e+08 1.12885365e+08]\n [1.18793275e+08 1.18278601e+08 1.17763927e+08 1.17249252e+08\n  1.16734578e+08 1.16219903e+08 1.15705229e+08 1.15190555e+08\n  1.14675880e+08 1.14161206e+08 1.13646531e+08]\n [1.19554433e+08 1.19039759e+08 1.18525084e+08 1.18010410e+08\n  1.17495736e+08 1.16981061e+08 1.16466387e+08 1.15951713e+08\n  1.15437038e+08 1.14922364e+08 1.14407689e+08]\n [1.20315587e+08 1.19800912e+08 1.19286238e+08 1.18771563e+08\n  1.18256889e+08 1.17742215e+08 1.17227540e+08 1.16712866e+08\n  1.16198191e+08 1.15683517e+08 1.15168843e+08]\n [1.21076737e+08 1.20562063e+08 1.20047388e+08 1.19532714e+08\n  1.19018040e+08 1.18503365e+08 1.17988691e+08 1.17474017e+08\n  1.16959342e+08 1.16444668e+08 1.15929993e+08]]\n[[10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# Task 6",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\n# Supply chain homework  - policy iteration\n\n\n# Set policy iteration parameters\nmax_policy_iter = 10_000  # Maximum number of policy iterations\nmax_value_iter = 10_000  # Maximum number of value iterations\n\n# Initialize Markov Decision Process model\nmaxCapacity = 40\nactions = range(0,10+1) #action space\nstates = range(0,maxCapacity+1) #state space\ndemands = range(0,41) #demand space\nrewards = reward_matrix  # Direct rewards per state\ngamma = 0.9  # discount factor\n\nmax_iter = 5\ndelta = 1e-400 \nV = np.zeros([41,11])\npi = np.zeros([41,11])\n\n\n\nfor i in range(max_policy_iter):\n    # Initial assumption: policy is stable\n    optimal_policy_found = True\n\n    # Policy evaluation\n    # Compute value for each state under current policy\n    for j in range(max_value_iter):\n        max_diff = 0  # Initialize max difference\n\n        for s in states:\n            for act in actions:\n                \n                for a in actions:\n                    max_val = 0\n                    # Compute state value\n                    val = rewards[s][act]  # Get direct reward\n                    for demand in demands:\n                        metDemand = min(s, demand); #compute met demand\n                        unmetDemand = max((demand-s), 0); #compute unmet demand\n        \n                        s_next = min(s-metDemand+a, maxCapacity) #update state\n        \n                        val += (1/5) * (gamma * V[s_next][act])  # Add discounted downstream values\n                        \n                    max_diff = max(max_diff, abs(val - V[s][act]))\n                    V[s][act] = val \n        if max_diff < delta:\n            break\n\n    # Policy iteration\n    # With updated state values, improve policy if needed\n    for s in states:\n        for act in actions:\n            val_max = V[s][act]\n            for a in actions:\n                val = rewards[s][act]  # Get direct reward\n                for demand in demands:\n                    metDemand = min(s, demand); #compute met demand\n                    unmetDemand = max((demand-s), 0); #compute unmet demand\n    \n                    s_next = min(s-metDemand+a, maxCapacity) #update state\n    \n                    val += (1/5) * (gamma * V[s_next][act])  # Add discounted downstream values\n    \n                # Update policy if (i) action improves value and (ii) action different from current policy\n                if val > val_max and pi[s][act] != a:\n                    pi[s][act] = a\n                    val_max = val\n                    optimal_policy_found = False\n\n    # If policy did not change, algorithm terminates\n    if optimal_policy_found:\n        break\n\nprint(V)\nprint(pi)\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}